{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codiv_19.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vj9S6PPl_pjM",
        "5xtJPGUGWl38",
        "Fnl8qj9QWrRm",
        "1fdxIKzXWwDC",
        "RfkKTAYNW1c1",
        "DLjXUeiJW6K3",
        "gJsm9TrnW-bC",
        "-TvvpuDdBYNR",
        "A_iRtDlpBe0D",
        "tWpVxok47a6-",
        "cv6ecQF_DnNl",
        "lh649HrJDGP5"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrMimic/Machine-Learning/blob/master/Codiv_19_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guk4Q9E_4gh",
        "colab_type": "text"
      },
      "source": [
        "## General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwxHgr-_8uy",
        "colab_type": "text"
      },
      "source": [
        "[trello board](https://trello.com/b/ED3H13vT/covid-19-kaggle-kickoff)\n",
        "\n",
        "forked [notebook](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings)\n",
        "\n",
        "List of interesting notebook:\n",
        "\n",
        "[Explored drugs being develloped](https://www.kaggle.com/maria17/cord-19-explore-drugs-being-developed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbfqgVO9Npa",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle dataset and word vectors downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2s1OVlL-QZ_",
        "colab_type": "text"
      },
      "source": [
        "Update your kaggle.json API key with the cell below, then launch the newt two.\n",
        "\n",
        "It'll download data if needed (*eg*, your kernel has restarted)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhf1L5gY_faQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload your kaggle API token\n",
        "from google.colab import files\n",
        "# In kaggle.com : MyAccount -> Create New API Token, will download kaggle.json that you can upload here.\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NouqN2I_H1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isfile(\"kaggle.json\") and not os.path.isdir(os.path.expanduser(\"~/.kaggle\")):\n",
        "  raise Exception(\"Please import your kaggle key first.\")\n",
        "\n",
        "if os.path.isfile(\"kaggle.json\"):\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "  !rm kaggle.json\n",
        "\n",
        "if not os.path.isdir(\"kaggle_data\"):\n",
        "  # !kaggle datasets list | head\n",
        "  !pip install -q kaggle\n",
        "  !pip install -q kaggle-cli\n",
        "\n",
        "  !kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge\n",
        "  !mkdir kaggle_data\n",
        "  !unzip -qq CORD-19-research-challenge.zip -d kaggle_data\n",
        "  !rm CORD-19-research-challenge.zip\n",
        "\n",
        "if not os.path.isdir(\"glove_vectors\"):\n",
        "  !pip install -q kaggle\n",
        "  !pip install -q kaggle-cli\n",
        "\n",
        "  !kaggle datasets download -d rtatman/glove-global-vectors-for-word-representation\n",
        "  !mkdir glove_vectors\n",
        "  !unzip -qq glove-global-vectors-for-word-representation.zip -d glove_vectors\n",
        "  !rm glove-global-vectors-for-word-representation.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVXJYpCcSafq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK data for pre-processing\n",
        "if not os.path.isdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "\n",
        "if \"corpora\" not in os.listdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "if \"tokenizers\" not in os.listdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "\n",
        "# Python packages\n",
        "try:\n",
        "  from retry import retry\n",
        "except ModuleNotFoundError:\n",
        "  !pip install retry\n",
        "\n",
        "try:\n",
        "  import pathos\n",
        "except ModuleNotFoundError:\n",
        "  !pip install pathos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEAuaVHJUdv7",
        "colab_type": "text"
      },
      "source": [
        "## Link your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PKGJlRMSV_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload files to your google drive (SQLite file eg) and mount it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKnw314-3cb",
        "colab_type": "text"
      },
      "source": [
        "## Libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj9S6PPl_pjM",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Fbnf7q9IiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import tqdm\n",
        "import time\n",
        "import pickle\n",
        "import sqlite3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "\n",
        "from retry import retry\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from dateutil import parser\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Generator\n",
        "from collections import OrderedDict, Counter, MutableMapping, Sequence\n",
        "from pathos.multiprocessing import ProcessingPool as picklable_pool\n",
        "from textblob import TextBlob\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xtJPGUGWl38",
        "colab_type": "text"
      },
      "source": [
        "### File processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ5UnYXjWmPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\" Open JSON file and return dict() data \"\"\"\n",
        "    with open(file_path, \"r\") as handler:\n",
        "        json_data = json.loads(handler.read(), object_pairs_hook=OrderedDict)\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def get_body(json_data: Dict[str, Any]) -> str:\n",
        "    \"\"\" Return body from json data \"\"\"\n",
        "    return \" \".join([json_data[\"body_text\"][index][\"text\"].strip() for index in range(len(json_data[\"body_text\"]))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnl8qj9QWrRm",
        "colab_type": "text"
      },
      "source": [
        "### Language detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8kpcSk9WrYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lang(text: str) -> str:\n",
        "  \"\"\" Detects language of text : must contain minimum 3 characters \"\"\"\n",
        "  if len(text) >= 3:\n",
        "    b = TextBlob(text)\n",
        "    return b.detect_language()\n",
        "  else:\n",
        "    raise ValueError('Minimum of 3 characters needed !')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fdxIKzXWwDC",
        "colab_type": "text"
      },
      "source": [
        "### Database utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjEeIyKaWwKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def instanciate_sql_db(db_path: str = \"articles_database.sqlite\") -> None:\n",
        "    \"\"\" Create an SQLite database \"\"\"\n",
        "\n",
        "    if os.path.isfile(db_path):\n",
        "        os.remove(db_path)    \n",
        "    database = sqlite3.connect(db_path)\n",
        "    # Storing articles\n",
        "    articles_table = {\n",
        "        \"id\": \"TEXT PRIMARY KEY\",\n",
        "        \"date\": \"DATETIME\",\n",
        "        \"body\": \"TEXT\",\n",
        "        \"abstract\": \"TEXT\",\n",
        "        \"title\": \"TEXT\",\n",
        "        \"sha\": \"TEXT\",\n",
        "        \"folder\": \"TEXT\"\n",
        "    }\n",
        "    columns = [\"{0} {1}\".format(name, col_type) for name, col_type in articles_table.items()]\n",
        "    command = \"CREATE TABLE IF NOT EXISTS articles ({});\".format(\", \".join(columns))\n",
        "    database.execute(command)\n",
        "    # Storing sentences\n",
        "    sentences_table = {\n",
        "        \"paper\": \"TEXT\",\n",
        "        \"section\": \"TEXT\",\n",
        "        \"sentence\": \"TEXT\",\n",
        "        \"vector\": \"TEXT\"\n",
        "    }\n",
        "    columns = [\"{0} {1}\".format(name, col_type) for name, col_type in sentences_table.items()]\n",
        "    command = \"CREATE TABLE IF NOT EXISTS sentences ({});\".format(\", \".join(columns))\n",
        "    database.execute(command)\n",
        "    database.close()\n",
        "\n",
        "def get_articles_to_insert(articles_df: pd.DataFrame) -> List[Any]:\n",
        "    \"\"\" List comprehension get stuck, who knows why \"\"\"\n",
        "    articles = []\n",
        "    for index, data in articles_df.iterrows():\n",
        "        articles.append((index, data))\n",
        "    return articles\n",
        "  \n",
        "@retry(sqlite3.OperationalError, tries=5, delay=2)\n",
        "def insert_row(list_to_insert: List[Any], table_name: str = \"articles\", db_path: str = \"articles_database.sqlite\") -> None:\n",
        "    \"\"\" Insert row of articles into the SQLite database \"\"\"\n",
        "\n",
        "    if table_name == \"articles\":\n",
        "        command = \"INSERT INTO articles(id, title, body, abstract, date, sha, folder) VALUES (?, ?, ?, ?, ?, ?, ?)\"\n",
        "    elif table_name == \"sentences\":\n",
        "        command = \"INSERT INTO sentences(paper, section, sentence, vector) VALUES (?, ?, ?, ?)\"\n",
        "    else:\n",
        "        raise Exception(f\"Unknown table {table_name}\")\n",
        "\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(command, list_to_insert)  # This line will be retried if fails\n",
        "    cursor.close()\n",
        "    connection.commit()\n",
        "    connection.close()\n",
        "\n",
        "def insert_article(args: Any) -> None:\n",
        "    \"\"\" Parse and insert a single article into the SQLite DB. args = [(index, df_line), db_path] \"\"\"\n",
        "    index = args[0][0]\n",
        "    data = args[0][1]\n",
        "    db_path = args[1]\n",
        "\n",
        "    # Get body\n",
        "    if data.has_full_text is True:\n",
        "        json_file = os.path.join(os.sep, \"kaggle\", \"input\", \"CORD-19-research-challenge\", data.full_text_file, data.full_text_file, f\"{data.sha}.json\")\n",
        "        try:\n",
        "            json_data = read_file(json_file)\n",
        "            body = get_body(json_data=json_data)\n",
        "            folder = data.full_text_file\n",
        "        except FileNotFoundError:\n",
        "            body = None\n",
        "            folder = None\n",
        "    else:\n",
        "        body = None\n",
        "        folder = None\n",
        "\n",
        "    try:\n",
        "        date = parser.parse(data.publish_time)\n",
        "    except Exception:  # Better to get no date than a string of whatever\n",
        "        date = None\n",
        "        \n",
        "    raw_data = [\n",
        "        data.doi,\n",
        "        data.title,\n",
        "        body,\n",
        "        data.abstract,\n",
        "        date,\n",
        "        data.sha,\n",
        "        folder\n",
        "    ]\n",
        "    insert_row(list_to_insert=raw_data, db_path=db_path)\n",
        "\n",
        "def get_all_ids(db_path: str = \"articles_database.sqlite\") -> List[str]:\n",
        "    \"\"\" Return all articles DOI stored in the article table \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT id FROM articles\")\n",
        "    ids = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "    ids_cleaneds = [id_[0] for id_ in ids if len(id_) == 1]\n",
        "\n",
        "    return ids_cleaneds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfkKTAYNW1c1",
        "colab_type": "text"
      },
      "source": [
        "### Text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AygcyhWW1ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text: str, stem_words: bool = True, remove_num: bool = True) -> List[str]:\n",
        "    \"\"\" Pre-process extracted texts \"\"\"\n",
        "\n",
        "    word = RegexpTokenizer(r\"\\w+\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    \n",
        "    def filter_stopwords(sentence: List[str], stopwords: List[str] = stop_words) -> List[str]:\n",
        "        \"\"\" Remove stopwords from a given list of words \"\"\"\n",
        "        return [word for word in sentence if word not in stopwords]\n",
        "    \n",
        "    def stem_words(sentence: List[str], stem_function: Any = stemmer) -> List[str]:\n",
        "        \"\"\" Get words root for every member of an input list \"\"\"\n",
        "        return [stem_function.stem(word) for word in sentence]\n",
        "    \n",
        "    def remove_numeric_words(sentence: List[str]) -> List[str]:\n",
        "        \"\"\" Remove number (items) from a list of words \"\"\"\n",
        "        letter_pattern = re.compile(r\"[a-z]\")\n",
        "        return [word for word in sentence if letter_pattern.match(word)]   \n",
        "\n",
        "    # Lower\n",
        "    paragraph = text.lower()\n",
        "    # Split paragraphs into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Split sentences into words and remove punctuation\n",
        "    sentences = [word.tokenize(sentence) for sentence in sentences]\n",
        "    # Remove stopwords\n",
        "    sentences = [filter_stopwords(sentence) for sentence in sentences]\n",
        "    if stem_words is True:\n",
        "        # Stem words\n",
        "        sentences = [stem_words(sentence) for sentence in sentences]\n",
        "    if remove_num is True:\n",
        "        sentences = [remove_numeric_words(sentence) for sentence in sentences]\n",
        "    # Filter empty sentences and one-letters words\n",
        "    sentences = [[word for word in sentence if len(word) > 1] for sentence in sentences if sentence != []]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def pre_process_articles(args: List[Any]) -> None:\n",
        "    \"\"\" Apply preprocessing to texts and store result into the SQLite DB \"\"\"\n",
        "\n",
        "    article_id: str = args[0]\n",
        "    embedding_model = args[1]\n",
        "    db_path: str = args[2]\n",
        "\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT * FROM articles WHERE id = ?\", [article_id])\n",
        "    # Get dict {column: value}\n",
        "    try:\n",
        "      article = {[col for col in head if col is not None][0]: value for head, value in zip(cursor.description, cursor.fetchone())}\n",
        "      cursor.close()\n",
        "      connection.close()\n",
        "    except TypeError:  # When the DB doest not return a result\n",
        "      cursor.close()\n",
        "      connection.close()\n",
        "      return None\n",
        "    \n",
        "    for section in [\"title\", \"abstract\", \"body\"]:\n",
        "        if article[section] is not None:\n",
        "            sentences = preprocess_text(article[section], stem_words=False, remove_num=False)\n",
        "            for sentence in sentences:\n",
        "              try:\n",
        "                # paper, section, sentence, vector\n",
        "                row_to_insert = [\n",
        "                    article_id,\n",
        "                    section,\n",
        "                    json.dumps(sentence),                                                               # Store list of tokens as loadable str\n",
        "                    json.dumps([str(x) for x in embedding_model.compute_sentence_vector(sentence)])     # Embeded vector\n",
        "                ]\n",
        "                insert_row(list_to_insert=row_to_insert, table_name=\"sentences\", db_path=db_path)\n",
        "              except TypeError:  # When all words are not in the model\n",
        "                continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLjXUeiJW6K3",
        "colab_type": "text"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfHSRlW1W6Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding():\n",
        "\n",
        "    def __init__(self, vectors_path: str = None, embeddings_dimension: int = 50, sentence_embedding_method: str = \"mowe\"):\n",
        "        \n",
        "        if vectors_path is None:\n",
        "            self.vectors_path = os.path.join(os.sep, \"kaggle\", \"input\", \"glove-global-vectors-for-word-representation\", f\"glove.6B.{embeddings_dimension}d.txt\")\n",
        "        else:\n",
        "            self.vectors_path = vectors_path\n",
        "        \n",
        "        self.embeddings_dimension = embeddings_dimension\n",
        "        self.sentence_embedding_method = sentence_embedding_method\n",
        "\n",
        "    def build_vectors_dictionary(self) -> Dict[str, List[float]]:\n",
        "        \"\"\" Load pre-trained vectors and build a dict \"\"\"\n",
        "\n",
        "        tic = time.time()    \n",
        "\n",
        "        self.vectors = {}\n",
        "        with open(self.vectors_path, \"r\") as handler:\n",
        "            for line in handler.readlines():\n",
        "                try:\n",
        "                    # Prevent to keep useless words (otherwise pre-proc return nothing)\n",
        "                    word = preprocess_text(line.split()[0])[0][0]\n",
        "                    vector = [float(dimension) for dimension in line.split()[1:None]]\n",
        "                    assert len(vector) == self.embeddings_dimension\n",
        "                    self.vectors[word] = vector\n",
        "                except IndexError:\n",
        "                    continue\n",
        "\n",
        "        toc = time.time()\n",
        "        print(f\"Took {round((toc-tic) / 60, 2)} min to load {len(self.vectors.keys())} GloVe vectors (embedding dim: {self.embeddings_dimension}).\")\n",
        "        \n",
        "    def compute_sentence_vector(self, sentence: List[str], sentence_embedding_method: str = \"mowe\") -> List[float]:\n",
        "        \"\"\" Compute a SOWE/MOWE over all tokens composing a sentence. Word skipped if not in model. \"\"\"\n",
        "        words_vector = [self.vectors[word] if word in self.vectors.keys() else list(list(np.full([1, EMBEDDING_DIMENSION, ], np.nan))[0]) for word in sentence]\n",
        "        if self.sentence_embedding_method == \"mowe\":\n",
        "            sentence_embedding = np.nanmean(words_vector, axis=0)\n",
        "        elif self.sentence_embedding_method == \"sowe\":\n",
        "            sentence_embedding = np.nansum(words_vector, axis=0)\n",
        "        else:\n",
        "            raise Exception(f\"No such sentence embedding method: {sentence_embedding_method}\")\n",
        "        return sentence_embedding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJsm9TrnW-bC",
        "colab_type": "text"
      },
      "source": [
        "### Query matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X35nczOW-h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_query(query: str) -> List[float]:\n",
        "    \"\"\" Vectorize a sentence \"\"\"\n",
        "    split_query = preprocess_text(query, stem_words=False, remove_num=False)[0]\n",
        "    query_vector = embedding_model.compute_sentence_vector(split_query)\n",
        "    return query_vector\n",
        "\n",
        "def get_sentences(db_path: str) -> List[Any]:\n",
        "    \"\"\" Retrieve all sentences \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    command = \"SELECT * FROM sentences\"\n",
        "    cursor.execute(command)\n",
        "    data = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "def compute_cosine_distance(args: Any) -> float:\n",
        "    \"\"\" Compute cosine distance between two embeded sentences \"\"\"\n",
        "    sentence_vector = args[1]\n",
        "    query_vector = args[0]\n",
        "    distance = 1 - cosine_similarity([query_vector], [sentence_vector])[0][0]\n",
        "\n",
        "    return (distance, sentence_vector)\n",
        "\n",
        "def query_db_for_sentence(db_path: str, vector: str):\n",
        "    \"\"\" Get a full sentence from a vector \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    command = \"SELECT * FROM sentences WHERE vector='%s'\" % vector\n",
        "    cursor.execute(command)\n",
        "    data = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    data = list(set(data))\n",
        "\n",
        "    if len(data) > 1:\n",
        "      print(f\"ERROR: two sentences with vector {vector} have been found.\")\n",
        "      data = None\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TvvpuDdBYNR",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1726qvpmBaDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIMENSION = 100\n",
        "SENTENCE_EMBEDDING_METHOD = \"mowe\"\n",
        "\n",
        "DB_FILE_NAME = os.path.join(f\"articles_database_v2_02042020_embedding_{EMBEDDING_DIMENSION}.sqlite\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_iRtDlpBe0D",
        "colab_type": "text"
      },
      "source": [
        "## Insert articles into sqlite DBÂ¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MbKnVdHBhN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_db_and_load_articles(db_path: str = \"articles_database.sqlite\", load_file: bool = True) -> None:\n",
        "    \"\"\" Load metadata.csv, try to get body texts and insert \"\"\"\n",
        "\n",
        "    if load_file is True:\n",
        "      assert os.path.isfile(db_path)\n",
        "      print(f\"DB {db_path} will be used instead.\")\n",
        "\n",
        "    else:\n",
        "      tic = time.time()\n",
        "\n",
        "      # The metadata.csv file will be used to fetch available files\n",
        "      metadata_path = os.path.join(os.sep, \"content\", \"kaggle_data\", \"metadata.csv\")\n",
        "      metadata_df = pd.read_csv(metadata_path)\n",
        "      # The DOI isn't unique, then let's keep the last version of a duplicated paper\n",
        "      metadata_df.drop_duplicates(subset=[\"doi\"], keep=\"last\", inplace=True)\n",
        "      # Load usefull information to be stored: id, title, body, abstract, date, sha, folder\n",
        "      articles_to_be_inserted = [(article, DB_FILE_NAME) for article in get_articles_to_insert(metadata_df)]\n",
        "      # Create a new SQLite DB file\n",
        "      instanciate_sql_db(db_path=DB_FILE_NAME)\n",
        "      # Parallelize articles insertion\n",
        "      with mp.Pool(os.cpu_count()) as pool:\n",
        "          pool.map(insert_article, articles_to_be_inserted)\n",
        "\n",
        "      toc = time.time()\n",
        "      print(f\"Took {round((toc-tic) / 60, 2)} min to insert {len(articles_to_be_inserted)} articles (SQLite DB: {db_path}).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8MJLIv4B1Fy",
        "colab_type": "code",
        "outputId": "88a9160a-14d1-4a47-f147-f4d39178611b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Change load_file to False if you want to create the DB for the first time\n",
        "create_db_and_load_articles(DB_FILE_NAME, load_file=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 6.39 min to insert 42440 articles (SQLite DB: articles_database_v2_02042020_embedding_100.sqlite).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpVxok47a6-",
        "colab_type": "text"
      },
      "source": [
        "## Load word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjqsCy_C7av9",
        "colab_type": "code",
        "outputId": "4a1a2a05-8ce1-4686-8310-c87cedc9304a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_model = Embedding(\n",
        "    vectors_path=os.path.join(os.sep, \"content\", \"glove_vectors\", f\"glove.6B.{EMBEDDING_DIMENSION}d.txt\"),\n",
        "    embeddings_dimension=EMBEDDING_DIMENSION,\n",
        "    sentence_embedding_method=SENTENCE_EMBEDDING_METHOD\n",
        ")\n",
        "embedding_model.build_vectors_dictionary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 1.52 min to load 330620 GloVe vectors (embedding dim: 100).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv6ecQF_DnNl",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process and vectorize texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKFh7aoClZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process_and_vectorize_texts(embedding_model: Embedding, db_path: str = \"articles_database.sqlite\", load_file: bool = True) -> None:\n",
        "    \"\"\" Apply pre-processing to all loaded articles \"\"\"\n",
        "\n",
        "    if load_file is True:\n",
        "      assert os.path.isfile(db_path)\n",
        "      print(f\"DB {db_path} will be used instead.\")\n",
        "\n",
        "    else:\n",
        "      tic = time.time()\n",
        "\n",
        "      # Get all previously inserted IDS as well as a pointer on embedding method\n",
        "      ids = [(id_, embedding_model, db_path) for id_ in get_all_ids(db_path=db_path)]\n",
        "      # For each title, abstract and body, pre-processed found data\n",
        "\n",
        "      with picklable_pool(os.cpu_count()) as pool:\n",
        "          pool.map(pre_process_articles, ids)\n",
        "\n",
        "      toc = time.time()\n",
        "      print(f\"Took {round((toc-tic) / 60, 2)} min to pre-process {len(ids)} articles (SQLite DB: {db_path}).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0Chs8CDp0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8763c23b-aba8-441d-bf5b-5f64961127ac"
      },
      "source": [
        "# Change load_file to False if you want to create the DB for the first time\n",
        "pre_process_and_vectorize_texts(embedding_model, DB_FILE_NAME, load_file=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: Mean of empty slice\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: Mean of empty slice\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Took 59.44 min to pre-process 42440 articles (SQLite DB: articles_database_v2_02042020_embedding_100.sqlite).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh649HrJDGP5",
        "colab_type": "text"
      },
      "source": [
        "## Query the DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAhZbpiiQRi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30bdeaa4-a2e8-4070-d66c-1a4c3a4c22ce"
      },
      "source": [
        "# Get sentences vectors to be matched with queries (stay in RAM, thus computed once).\n",
        "sentences = get_sentences(db_path=DB_FILE_NAME)\n",
        "sentences_vectors = [[float(x) for x in json.loads(sentence_vector[3])] for sentence_vector in sentences]\n",
        "sentences_vectors = [vector for vector in sentences_vectors if np.nansum(vector) != 0]\n",
        "\n",
        "print(f\"Queries will be matched versus {len(sentences_vectors)} vectors.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Queries will be matched versus 327426 vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRuNzYiODFz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1c384879-15aa-4cdf-deeb-a5023a2924a9"
      },
      "source": [
        "tic = time.time()\n",
        "\n",
        "# Will be replaced by a textbox\n",
        "query = \"chloroquine usage coronavirus treatment\"\n",
        "\n",
        "# Vectorize it and format as arguments to be mapped by mp.Pool\n",
        "query_vector = list(vectorize_query(query))\n",
        "mapping_arguments = [(query_vector, sentence_vector) for sentence_vector in sentences_vectors]\n",
        "\n",
        "# Execute\n",
        "with mp.Pool(os.cpu_count()) as pool:\n",
        "  distances_and_vectors = pool.map(compute_cosine_distance, mapping_arguments)\n",
        "\n",
        "# Get results\n",
        "distances = [item[0] for item in distances_and_vectors]\n",
        "vectors = [item[1] for item in distances_and_vectors]\n",
        "\n",
        "# Find closest\n",
        "closest_sentence_index = distances.index(min(distances))\n",
        "closest_vector = vectors[closest_sentence_index]\n",
        "closest_vector_str = json.dumps([str(x) for x in closest_vector])\n",
        "\n",
        "# Retrieve closest sentence\n",
        "closest_sentence = query_db_for_sentence(vector=closest_vector_str, db_path=DB_FILE_NAME)\n",
        "\n",
        "toc = time.time()\n",
        "\n",
        "for sentence in closest_sentence:\n",
        "  print(f\"DOI:\\t\\t\\t{sentence[0]}\")\n",
        "  print(f\"SECTION:\\t\\t{sentence[1]}\")\n",
        "  print(f\"SENTENCE:\\t\\t{sentence[2]}\")\n",
        "  print(f\"VECTOR:\\t\\t\\t{str(sentence[3])}\")\n",
        "  print(f\"TIME:\\t\\t\\t{round(toc - tic, 2)}\")\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DOI:\t\t\t10.3760/cma.j.issn.1001-0939.2020.0019\n",
            "SECTION:\t\ttitle\n",
            "SENTENCE:\t\t[\"Expert\", \"consensus\", \"chloroquine\", \"phosphate\", \"treatment\", \"novel\", \"coronavirus\", \"pneumonia\"]\n",
            "VECTOR:\t\t\t[\"-0.10206371428571429\", \"0.14450642857142856\", \"-0.11673999999999998\", \"0.2449471428571428\", \"-0.09503714285714285\", \"-0.06895071428571427\", \"0.2032712714285714\", \"-0.49277042857142855\", \"0.19871957142857144\", \"0.07751585714285714\", \"-0.21958442857142857\", \"0.055541999999999994\", \"0.09598342857142857\", \"-0.04432700000000001\", \"0.1801637142857143\", \"0.16684000000000002\", \"-0.3738714285714285\", \"-0.19058114285714287\", \"0.30563571428571434\", \"-0.29871455714285716\", \"-0.04745085714285713\", \"0.027170142857142865\", \"-0.27736802857142856\", \"0.13388771428571428\", \"0.031063\", \"0.2810084285714286\", \"0.36096885714285715\", \"-0.059184285714285696\", \"-0.26887742857142855\", \"0.0929812857142857\", \"0.16334857142857143\", \"0.18748\", \"-0.007528571428571456\", \"-0.19825714285714283\", \"0.09999771428571427\", \"-0.001404958571428559\", \"-0.1331617142857143\", \"-0.40814485714285714\", \"-0.06580949999999999\", \"0.08229571428571428\", \"0.05642999999999999\", \"0.3716754285714286\", \"0.16066585714285717\", \"-0.40037897142857143\", \"0.12525357142857144\", \"0.21429742857142856\", \"0.2722397142857143\", \"0.2340745714285714\", \"0.1419507142857143\", \"0.26800642857142853\", \"-0.02751842857142858\", \"0.22209128571428569\", \"-0.28821171428571424\", \"-0.1047435\", \"0.16218857142857143\", \"0.4263885714285715\", \"-0.022487142857142844\", \"-0.04921714285714285\", \"-0.3614\", \"-0.1440762857142857\", \"-0.12118514285714288\", \"0.26218442857142854\", \"0.10004171428571428\", \"-0.011394014285714278\", \"-0.282593\", \"0.33802571428571426\", \"-0.200761\", \"-0.4258457142857143\", \"0.07363428571428572\", \"-0.4071608571428572\", \"-0.2856204285714286\", \"-0.02446142857142857\", \"-0.12884714285714285\", \"0.5546542857142857\", \"0.06314314285714287\", \"-0.07274256285714285\", \"-0.14113257142857139\", \"-0.07563432857142859\", \"0.12536571428571433\", \"0.015312571428571422\", \"-0.3139367142857143\", \"0.2602588571428572\", \"0.1099727142857143\", \"0.2982771428571428\", \"0.03058228571428572\", \"0.22570570000000004\", \"0.2119504285714286\", \"0.21131514285714284\", \"-0.7154737142857143\", \"0.31619857142857144\", \"0.05404714285714285\", \"0.16035142857142856\", \"0.226505\", \"-0.03812428571428571\", \"0.5138414285714286\", \"-0.006275714285714321\", \"0.24371428571428572\", \"-0.057771871428571445\", \"-0.10323748571428572\", \"-0.28521199999999997\"]\n",
            "TIME:\t\t\t84.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8PP1aNFHuKq",
        "colab_type": "text"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HTSDId-LFIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "85df480d-6d82-49a8-efb9-91a5380fb645"
      },
      "source": [
        "connection = sqlite3.connect(DB_FILE_NAME)\n",
        "cursor = connection.cursor()\n",
        "cursor.execute(\"SELECT * FROM sentences\")\n",
        "res = cursor.fetchall()\n",
        "cursor.close()\n",
        "connection.close()\n",
        "\n",
        "print(len(res))\n",
        "print(res[42440])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "344867\n",
            "('10.1016/j.bbamcr.2014.08.004', 'abstract', '[\"The\", \"infected\", \"host\", \"cell\", \"detects\", \"trace\", \"amounts\", \"viral\", \"RNA\", \"last\", \"years\", \"revealed\", \"common\", \"principles\", \"biochemical\", \"mechanisms\", \"leading\", \"signal\", \"amplification\", \"required\", \"mounting\", \"powerful\", \"antiviral\", \"response\"]', '[\"0.16382169545454547\", \"0.023354745454545446\", \"0.013891272727272698\", \"0.12843163636363636\", \"-0.16910123636363633\", \"-0.020394590909090904\", \"-0.10999465454545455\", \"-0.19730999999999999\", \"0.13107109090909094\", \"0.20234877590909092\", \"-0.13380909545454542\", \"0.0007616318181818354\", \"0.08235004545454545\", \"-0.13349909090909093\", \"0.29918350000000005\", \"0.12036464545454549\", \"-0.2348821363636363\", \"0.05956048181818181\", \"0.129445\", \"-0.20799213636363637\", \"-0.21862849999999998\", \"-0.29948413636363636\", \"-0.1809764545454546\", \"-0.06356311818181816\", \"-0.12140527272727274\", \"-0.014608181818181819\", \"0.23308814545454545\", \"0.12170824090909091\", \"-0.13597333636363634\", \"0.03156654545454544\", \"0.07156563636363637\", \"0.1202871454545455\", \"-0.13269528181818183\", \"0.03234036363636364\", \"0.08615718181818183\", \"-0.17996454545454546\", \"-0.061167863636363624\", \"-0.25977413636363633\", \"-0.09126828636363635\", \"0.09386291818181816\", \"-0.024972768181818185\", \"0.1908551818181818\", \"-0.05755422727272728\", \"-0.13421895454545452\", \"0.044076636363636366\", \"0.08332700909090907\", \"0.12210459090909093\", \"0.018728027272727273\", \"-0.22909827272727268\", \"-0.1736465863636364\", \"0.26303559090909095\", \"0.10160163636363634\", \"0.08691713636363638\", \"0.13259854545454544\", \"0.21726981818181815\", \"-0.25778704545454545\", \"0.03722186363636364\", \"-0.2830365\", \"0.45438509090909085\", \"-0.0618328\", \"0.08025254545454544\", \"0.09759068181818184\", \"0.12898733636363635\", \"0.07386904545454547\", \"0.39058040909090913\", \"0.14931850000000002\", \"-0.198253655\", \"-0.3058876818181819\", \"0.1664009090909091\", \"-0.2350994090909091\", \"-0.011370590909090894\", \"-0.10162054545454545\", \"0.19557280909090913\", \"0.21121913636363632\", \"0.1217990409090909\", \"0.022579363636363637\", \"-0.1066264531818182\", \"-0.2790368181818182\", \"-0.18533854545454542\", \"-0.09342777272727272\", \"0.2160606409090909\", \"0.007148727272727271\", \"-0.29166935\", \"0.04835472727272728\", \"-0.7077440454545457\", \"0.21724854545454553\", \"0.45489498181818194\", \"0.05906881818181819\", \"-0.354971\", \"0.04470709090909094\", \"-0.2452014545454545\", \"0.06099113636363635\", \"-0.06599888181818182\", \"0.005305954545454527\", \"0.2291607045454545\", \"-0.12395913181818186\", \"0.011430559090909091\", \"-0.42920695454545454\", \"0.11799971363636365\", \"0.0006207227272727121\"]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873WZeEDEeDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_1 = [\"cool\", \"concert\", \"guitar\"]\n",
        "sentence_2 = [\"super\", \"piano\", \"song\"]\n",
        "sentence_3 = [\"boat\", \"drugs\", \"corona\"]\n",
        "\n",
        "distance_1_2 = compute_cosine_distance(\n",
        "    embeding_model.compute_sentence_vector(sentence_1),\n",
        "    embeding_model.compute_sentence_vector(sentence_2)\n",
        ")\n",
        "\n",
        "distance_1_3 = compute_cosine_distance(\n",
        "    embeding_model.compute_sentence_vector(sentence_1),\n",
        "    embeding_model.compute_sentence_vector(sentence_3)\n",
        ")\n",
        "\n",
        "print(f\"Distance between: '{' '.join(sentence_1)}' and '{' '.join(sentence_2)}': {distance_1_2}\")\n",
        "print(f\"Distance between: '{' '.join(sentence_1)}' and '{' '.join(sentence_3)}': {distance_1_3}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}